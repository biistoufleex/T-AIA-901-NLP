{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.training.example import offsets_to_biluo_tags\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "from spacy.util import minibatch, compounding\n",
    "from unidecode import unidecode\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les phrases d'entrée\n",
    "input_data = [\n",
    "\"1,je voudrais aller de Toulouse à bordeaux\",\n",
    "\"2,Comment me rendre à Port-Boulet depuis la gare de Tours ?\",\n",
    "\"3,Je veux aller voir mon ami Albert à Tours en partant de Bordeaux\",\n",
    "\"4,Il y a-t-il des trains de Nantes à Montaigu\",\n",
    "\"5,Une phrase sans origine ni destination\",\n",
    "\"Si pas de numéro de séquence, on considère que c'est zéro\",\n",
    "\"7, is there any train going from Paris to Marseille ?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des mots pour reconnaître la destination en première\n",
    "destination_keywords = [\n",
    "    \"à\",\n",
    "    \"vers\",\n",
    "    \"jusqu'à\",\n",
    "    \"pour\",\n",
    "    \"en direction de\",\n",
    "    \"jusqu'à\",\n",
    "    # ... (ajoutez d'autres expressions si nécessaire)\n",
    "] \n",
    "\n",
    "# Liste des mots pour reconnaître la departure en première\n",
    "departure_keywords = [\n",
    "    \"depuis\",\n",
    "    \"de\",\n",
    "    \"au départ de\",\n",
    "    \"en partant de\",\n",
    "    \"depuis\",\n",
    "    \"au départ de chez\",\n",
    "    # ... (ajoutez d'autres expressions si nécessaire)\n",
    "]\n",
    "# Liste des mots pour reconnaître la departure en première\n",
    "passage_keywords = [\n",
    "    \"passant par\",\n",
    "    \"via\",\n",
    "    \"par le chemin de\",\n",
    "    \"par la route de\",\n",
    "    \"par la voie de\",\n",
    "    \"en faisant un détour par\",\n",
    "    \"par l'intermédiaire de\",\n",
    "    \"en incluant\",\n",
    "    \"avec un arrêt à\",\n",
    "    \"parmi les étapes à\",\n",
    "    \"en traversant\",\n",
    "    \"en faisant escale à\",\n",
    "    \"tout en visitant\",\n",
    "    \"parmi les destinations à\",\n",
    "    \"et en découvrant\",\n",
    "    \"tout en passant par\",\n",
    "    \"tout en explorant\",\n",
    "    \"avec un passage à\",\n",
    "    \"en chemin vers\",\n",
    "    \"en voyage vers\",\n",
    "    # Ajoutez d'autres expressions si nécessaire\n",
    "]\n",
    "\n",
    "# Liste des mots a exclure\n",
    "exclude_words = [\n",
    "    \"gare\",\n",
    "    \"Gare\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour détecter la langue de la phrase\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        if language != 'fr':   \n",
    "            return True  # Vérifie si la langue détectée n'est pas le français\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les informations pertinentes\n",
    "def extract_trip_info(text):\n",
    "\n",
    "    for mot in exclude_words:\n",
    "        text = text.replace(mot, '')\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    locs = []\n",
    "    result = []\n",
    "    \n",
    "    is_not_french = detect_language(text)\n",
    "    if is_not_french:\n",
    "        locs.append(\"NOT_FRENCH\")\n",
    "        return locs\n",
    " \n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"LOC\":\n",
    "            locs.append(token.text)\n",
    "    print(locs)\n",
    "    if not locs:  # Si la liste est vide\n",
    "            result.append('NOT_TRIP')\n",
    "\n",
    "    # Normalisation des mots-clés des destination\n",
    "    destination_keywords_normalized = [unidecode(keyword) for keyword in destination_keywords]\n",
    "\n",
    "    # Normalisation des mots-clés des depart\n",
    "    departure_keywords_normalized = [unidecode(keyword) for keyword in departure_keywords]\n",
    "\n",
    "   # Normalisation des mots-clés des passage\n",
    "    passage_keywords_normalized = [unidecode(keyword) for keyword in passage_keywords]\n",
    "\n",
    "\n",
    "    departure = None\n",
    "    destination = None\n",
    "    passage = None\n",
    "    departure_count = 0\n",
    "    destination_count = 0\n",
    "    passage_count = 0\n",
    "\n",
    "\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        # use unicode for text\n",
    "        text_unicode = token.text\n",
    "        if token.text in departure_keywords or text_unicode in departure_keywords or token.text in departure_keywords_normalized or text_unicode in departure_keywords_normalized and i < len(doc) - 1:\n",
    "            if doc[i + 1].text in locs:\n",
    "                departure = doc[i+1].text\n",
    "                destination_count += 1  \n",
    "    for i, token in enumerate(doc):\n",
    "        # use unicode for text\n",
    "        text_unicode = unidecode(token.text)\n",
    "        if token.text in destination_keywords or text_unicode in destination_keywords or token.text in destination_keywords_normalized or text_unicode in destination_keywords_normalized and i > 0:\n",
    "            if doc[i + 1].text in locs:\n",
    "                destination = doc[i+1].text\n",
    "                departure_count += 1\n",
    "\n",
    "    # for i, token in enumerate(doc):\n",
    "    #     # use unicode for text\n",
    "    #     text_unicode = unidecode(token.text)\n",
    "    #     print(token.text)\n",
    "    #     if token.text in passage_keywords or text_unicode in passage_keywords or token.text in passage_keywords_normalized or text_unicode in passage_keywords_normalized and i < len(doc) - 1:\n",
    "    #         if doc[i + 1].text in locs:\n",
    "    #             passage = doc[i+1].text\n",
    "    #             passage_count += 1  \n",
    "\n",
    "\n",
    "    for keyword in passage_keywords:\n",
    "        keyword_words = keyword.split()  # Divise l'expression-clé en mots\n",
    "        keyword_length = len(keyword_words)\n",
    "\n",
    "        for i in range(len(doc) - keyword_length + 1):\n",
    "            window = [token.text for token in doc[i:i+keyword_length]]\n",
    "\n",
    "            if window == keyword_words:\n",
    "                print(window, keyword)\n",
    "                passage_start = i + keyword_length\n",
    "                passage_end = passage_start + 1\n",
    "                if doc[passage_start:passage_end].text in locs:\n",
    "                    passage = doc[passage_start:passage_end].text\n",
    "                    passage_count += 1 \n",
    "                    break\n",
    "\n",
    "    if departure and destination and departure_count == 1 and destination_count == 1 and passage_count == 0 :\n",
    "        result = {\"DEPARTURE\": departure, \"DESTINATION\": destination}\n",
    "    elif departure and destination and passage and departure_count == 1 and destination_count == 1 and passage_count == 1:\n",
    "        result = {\"DEPARTURE\": departure,\"BETWEEN\" : passage, \"DESTINATION\": destination}\n",
    "    if (departure_count > 1) or (destination_count > 1):\n",
    "        result = {'NOT_TRIP'}\n",
    "        \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Toulouse', 'Paris', 'Limoges']\n",
      "['passant', 'par'] passant par\n",
      "1\n",
      "1\n",
      "1\n",
      "{'DEPARTURE': 'Toulouse', 'BETWEEN': 'Limoges', 'DESTINATION': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(extract_trip_info(\"Je voudrais aller de Toulouse à Paris en passant par Limoges\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "20\n",
      "26\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "text = \"On bouge de Bordeaux pour Lyon ce soir, c'est ok pour toi ?\"\n",
    "\n",
    "print(text.find(\"Bordeaux\"))\n",
    "print((text.find(\"Bordeaux\")+ len(\"Bordeaux\")))\n",
    "\n",
    "print(text.find(\"Lyon\"))\n",
    "print((text.find(\"Lyon\")+ len(\"Lyon\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
