{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.training.example import offsets_to_biluo_tags\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "from spacy.util import minibatch, compounding\n",
    "from unidecode import unidecode\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les phrases d'entrée\n",
    "input_data = [\n",
    "\"1,je voudrais aller de Toulouse à bordeaux\",\n",
    "\"2,Comment me rendre à Port-Boulet depuis la gare de Tours ?\",\n",
    "\"3,Je veux aller voir mon ami Albert à Tours en partant de Bordeaux\",\n",
    "\"4,Il y a-t-il des trains de Nantes à Montaigu\",\n",
    "\"5,Une phrase sans origine ni destination\",\n",
    "\"Si pas de numéro de séquence, on considère que c'est zéro\",\n",
    "\"7, is there any train going from Paris to Marseille ?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des mots pour reconnaître la destination en première\n",
    "destination_keywords = [\n",
    "    \"à\",    \n",
    "    \"vers\",\n",
    "    \"jusqu'à\",\n",
    "    \"pour\",\n",
    "    \"en direction de\",\n",
    "    \"jusqu'à\",\n",
    "    \"atteindre\"\n",
    "    # ... (ajoutez d'autres expressions si nécessaire)\n",
    "] \n",
    "\n",
    "# Liste des mots pour reconnaître la departure en première\n",
    "departure_keywords = [\n",
    "    \"depuis\",\n",
    "    \"de\",\n",
    "    \"au départ de\",\n",
    "    \"au\",\n",
    "    \"en partant de\",\n",
    "    \"depuis\",\n",
    "    \"au départ de chez\",\n",
    "    \"départ\"\n",
    "    # ... (ajoutez d'autres expressions si nécessaire)\n",
    "]\n",
    "# Liste des mots pour reconnaître la departure en première\n",
    "passage_keywords = [\n",
    "    \"passant par\",\n",
    "    \"via\",\n",
    "    \"par le chemin de\",\n",
    "    \"par la route de\",\n",
    "    \"par la voie de\",\n",
    "    \"en faisant un détour par\",\n",
    "    \"par l'intermédiaire de\",\n",
    "    \"en incluant\",\n",
    "    \"avec un arrêt à\",\n",
    "    \"parmi les étapes à\",\n",
    "    \"en traversant\",\n",
    "    \"en faisant escale à\",\n",
    "    \"tout en visitant\",\n",
    "    \"parmi les destinations à\",\n",
    "    \"et en découvrant\",\n",
    "    \"tout en passant par\",\n",
    "    \"tout en explorant\",\n",
    "    \"avec un passage à\",\n",
    "    \"en chemin vers\",\n",
    "    \"en voyage vers\",\n",
    "    # Ajoutez d'autres expressions si nécessaire\n",
    "]\n",
    "\n",
    "# Liste des mots a exclure\n",
    "exclude_words = [\n",
    "    \"gare\",\n",
    "    \"Gare\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour détecter la langue de la phrase\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        if language != 'fr':   \n",
    "            return True  # Vérifie si la langue détectée n'est pas le français\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les informations pertinentes\n",
    "def extract_trip_info(text):\n",
    "\n",
    "    for mot in exclude_words:\n",
    "        text = text.replace(mot, '')\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    locs = []\n",
    "    result = []\n",
    "    \n",
    "    is_not_french = detect_language(text)\n",
    "    if is_not_french:\n",
    "        locs.append(\"NOT_FRENCH\")\n",
    "        return locs\n",
    " \n",
    "    for token in doc:\n",
    "\n",
    "        if token.ent_type_ == \"LOC\":\n",
    "            locs.append(token.text)\n",
    "    print(locs)\n",
    "    if not locs:  # Si la liste est vide\n",
    "            result.append('NOT_TRIP')\n",
    "\n",
    "    # Normalisation des mots-clés des destination\n",
    "    destination_keywords_normalized = [unidecode(keyword) for keyword in destination_keywords]\n",
    "\n",
    "    # Normalisation des mots-clés des depart\n",
    "    departure_keywords_normalized = [unidecode(keyword) for keyword in departure_keywords]\n",
    "\n",
    "   # Normalisation des mots-clés des passage\n",
    "    passage_keywords_normalized = [unidecode(keyword) for keyword in passage_keywords]\n",
    "\n",
    "\n",
    "    departure = None\n",
    "    destination = None\n",
    "    passage = None\n",
    "    departure_count = 0\n",
    "    destination_count = 0\n",
    "    passage_count = 0\n",
    "\n",
    "\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        # use unicode for text\n",
    "        text_unicode = token.text\n",
    "        if token.text in departure_keywords or text_unicode in departure_keywords or token.text in departure_keywords_normalized or text_unicode in departure_keywords_normalized and i < len(doc) - 1:\n",
    "            if doc[i + 1].text in locs:\n",
    "                departure = doc[i+1].text\n",
    "                destination_count += 1  \n",
    "    for i, token in enumerate(doc):\n",
    "        # use unicode for text\n",
    "        text_unicode = unidecode(token.text)\n",
    "        if token.text in destination_keywords or text_unicode in destination_keywords or token.text in destination_keywords_normalized or text_unicode in destination_keywords_normalized and i > 0:\n",
    "            if doc[i + 1].text in locs:\n",
    "                destination = doc[i+1].text\n",
    "                departure_count += 1\n",
    "\n",
    "    # for i, token in enumerate(doc):\n",
    "    #     # use unicode for text\n",
    "    #     text_unicode = unidecode(token.text)\n",
    "    #     print(token.text)\n",
    "    #     if token.text in passage_keywords or text_unicode in passage_keywords or token.text in passage_keywords_normalized or text_unicode in passage_keywords_normalized and i < len(doc) - 1:\n",
    "    #         if doc[i + 1].text in locs:\n",
    "    #             passage = doc[i+1].text\n",
    "    #             passage_count += 1  \n",
    "\n",
    "\n",
    "    for keyword in passage_keywords:\n",
    "        keyword_words = keyword.split()  # Divise l'expression-clé en mots\n",
    "        keyword_length = len(keyword_words)\n",
    "\n",
    "        for i in range(len(doc) - keyword_length + 1):\n",
    "            window = [token.text for token in doc[i:i+keyword_length]]\n",
    "\n",
    "            if window == keyword_words:\n",
    "                print(window, keyword)\n",
    "                passage_start = i + keyword_length\n",
    "                passage_end = passage_start + 1\n",
    "                if doc[passage_start:passage_end].text in locs:\n",
    "                    passage = doc[passage_start:passage_end].text\n",
    "                    passage_count += 1 \n",
    "                    break\n",
    "\n",
    "    if departure and destination and departure_count == 1 and destination_count == 1 and passage_count == 0 :\n",
    "        result = {\"DEPARTURE\": departure, \"DESTINATION\": destination}\n",
    "    elif departure and destination and passage and departure_count == 1 and destination_count == 1 and passage_count == 1:\n",
    "        result = {\"DEPARTURE\": departure,\"BETWEEN\" : passage, \"DESTINATION\": destination}\n",
    "    if (departure_count > 1) or (destination_count > 1):\n",
    "        result = {'NOT_TRIP'}\n",
    "    elif (departure_count == 0) and (destination_count == 0):\n",
    "        result = {'NOT_TRIP'}\n",
    "        \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'Paris']\n",
      "{'DEPARTURE': 'Paris', 'DESTINATION': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(extract_trip_info(\"Je voudrais aller de toulouse à paris en passant par limoges\"))\n",
    "# print(extract_trip_info(\"j'aimerais partir chez Albert à Toulon depuis amiens\"))\n",
    "print(extract_trip_info(\"j'aimerais aller a Paris en partant de Paris\"))\n",
    "# print(extract_trip_info(\"bonjour, je veux zebi Paris nique Marseille\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Y a-t-il un moyen d'atteindre nantes depuis nevers ?\", {'entities': [(30, 36, 'ARRIVER'), (44, 50, 'DEPART')]})\n"
     ]
    }
   ],
   "source": [
    "text = \"On bouge de Bordeaux pour Lyon ce soir, c'est ok pour toi ?\"\n",
    "text2 = \"Y a-t-il un moyen d'atteindre nantes depuis nevers ?\"\n",
    "# text2 = \"Y a-t-il un moyen d'atteindre clermont depuis Lille ?\"\n",
    "# text2 = \"Trouve-moi un moyen de transport de nimes jusqu'à nevers.\"\n",
    "# text2 = \"Trouve-moi un itinéraire pour aller de montpellier à pau.\"\n",
    "# text2 = \"Rendez-vous à dax depuis biaritz.\"\n",
    "# text2 = \"Recherche le chemin entre colmar et bordeaux.\"\n",
    "# text2 = \"Je veux prendre un train de lyon à Bordeaux\"\n",
    "# text2 = \"Je veux un billet niort, Bourges\"\n",
    "# text2 = \"Je veux un billet poitiers Niort\"\n",
    "# text2 = \"De mulhouse vers la rochelle\"\n",
    "# text2 = \"Je veux faire strasbourg toulon comment puis-je faire ?\"\n",
    "# text2 = \"départ de châteauroux vers nantes\"\n",
    "# text2 = \"reims annecy\"\n",
    "# text2 = \"Rennes Lyon\"\n",
    "# text2 = \"Strasbourg paris\"\n",
    "# text2 = \"On me demande d'arriver a grenoble depuis tours\"\n",
    "# text2 = \"Je veux un billet de train pour paris lyon\"\n",
    "# text2 = \"j'aimerais venir a caen en partant de nice\"\n",
    "# text2 = \"Je veux me rendre a nimes\"\n",
    "# text2 = \"j'aimerais partir pour le havre depuis amiens\"\n",
    "# text2 = \"bonjour, je veux partir a rouen depuis marseille\"\n",
    "\n",
    "# print(text.find(\"Bordeaux\"))\n",
    "# print((text.find(\"Bordeaux\")+ len(\"Bordeaux\")))\n",
    "\n",
    "# print(text.find(\"Lyon\"))\n",
    "# print((text.find(\"Lyon\")+ len(\"Lyon\")))\n",
    "ville_depart = \"nevers\"\n",
    "ville_arriver = \"nantes\"\n",
    "\n",
    "# print(text2.find(ville_arriver))\n",
    "# print((text2.find(ville_arriver)+ len(ville_arriver)))\n",
    "# print(text2.find(ville_depart))\n",
    "# print((text2.find(ville_depart)+ len(ville_depart)))\n",
    "data = {\"entities\": [(text2.find(ville_arriver), (text2.find(ville_arriver)+ len(ville_arriver)), \"ARRIVER\"), (text2.find(ville_depart), (text2.find(ville_depart)+ len(ville_depart)), \"DEPART\")]}\n",
    "# print(data)\n",
    "print(f'(\"{text2}\", {data})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
